{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebadff9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ab6ab51a8e4daf895e51827b81dc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(crawl='CC-MAIN-2015-32', url_host_name='www.justincase.be', url_host_registered_domain='justincase.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438044160065.87/warc/CC-MAIN-20150728004240-00006-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.justincase.be/', warc_record_offset=532355587, warc_record_length=4905, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.hotel-leopold.be', url_host_registered_domain='hotel-leopold.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042987775.70/warc/CC-MAIN-20150728002307-00076-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.hotel-leopold.be/', warc_record_offset=504234759, warc_record_length=7503, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.thedeafnun.be', url_host_registered_domain='thedeafnun.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042986022.41/warc/CC-MAIN-20150728002306-00301-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.thedeafnun.be/', warc_record_offset=742777721, warc_record_length=20000, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.gengeavia.be', url_host_registered_domain='gengeavia.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042986022.41/warc/CC-MAIN-20150728002306-00181-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.gengeavia.be/', warc_record_offset=469189749, warc_record_length=8692, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.jakency.be', url_host_registered_domain='jakency.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042987174.71/warc/CC-MAIN-20150728002307-00036-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.jakency.be/', warc_record_offset=522417694, warc_record_length=7032, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.berlitz.be', url_host_registered_domain='berlitz.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042989897.84/warc/CC-MAIN-20150728002309-00226-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.berlitz.be/', warc_record_offset=323212497, warc_record_length=6573, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.labeur.be', url_host_registered_domain='labeur.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042989891.18/warc/CC-MAIN-20150728002309-00212-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.labeur.be/', warc_record_offset=544925214, warc_record_length=2797, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.bhs.be', url_host_registered_domain='bhs.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042989043.35/warc/CC-MAIN-20150728002309-00197-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.bhs.be/', warc_record_offset=334561035, warc_record_length=5129, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.dancingdog.be', url_host_registered_domain='dancingdog.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042989891.18/warc/CC-MAIN-20150728002309-00015-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.dancingdog.be/', warc_record_offset=395629436, warc_record_length=3629, ranking=1), Row(crawl='CC-MAIN-2015-32', url_host_name='www.ccecrb.fgov.be', url_host_registered_domain='fgov.be', warc_filename='crawl-data/CC-MAIN-2015-32/segments/1438042987552.57/warc/CC-MAIN-20150728002307-00296-ip-10-236-191-2.ec2.internal.warc.gz', url='http://www.ccecrb.fgov.be/', warc_record_offset=353777043, warc_record_length=1947, ranking=1)]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "input_bucket = 's3://zhan-commoncrawl/RQX/all_urls/'\n",
    "# input_bucket = \"s3://zhan-commoncrawl/DK_websites/dk_2021_websites/\"\n",
    "# df = spark.read.csv(input_bucket,header = False).toDF(\"crawl\",\"url_host_name\",\"url_host_registered_domain\",\"warc_filename\",\"url\",\"warc_record_offset\",\"warc_record_length\"\n",
    "# )\n",
    "df = spark.read.parquet(input_bucket)\n",
    "\n",
    "a = 'CC-MAIN-2015-32'\n",
    "df.createOrReplaceTempView(\"education_file_list\")\n",
    "sqlDF = spark.sql('SELECT * from education_file_list where crawl=\"{}\"'.format(a))\n",
    "# sqlDF.show()\n",
    "warc_filename_list = sqlDF.collect()[:10]\n",
    "\n",
    "print(warc_filename_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ead27",
   "metadata": {},
   "source": [
    "## 获得trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d488f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b2dc6ea63e4b8c8cee16feefc17f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+\n",
      "|              Domain|            Category|    Company|\n",
      "+--------------------+--------------------+-----------+\n",
      "|google-analytics.com|           Analytics|     Google|\n",
      "|googlesyndication...|         Advertising|     Google|\n",
      "|          google.com|                   #|     Google|\n",
      "|      googleapis.com|                   #|     Google|\n",
      "|        facebook.com|         Advertising|   Facebook|\n",
      "|      macromedia.com|                   #|      Adobe|\n",
      "|         youtube.com|                   #|     Google|\n",
      "|           adobe.com|                   #|      Adobe|\n",
      "|        facebook.net|         Advertising|   Facebook|\n",
      "|         twitter.com|         Advertising|    Twitter|\n",
      "|     casalemedia.com|         Advertising|CasaleMedia|\n",
      "|         addthis.com|              Widget|    AddThis|\n",
      "|        gravatar.com|              Widget| Automattic|\n",
      "|            gmpg.org|                   #|          #|\n",
      "|          imgaft.com|                   #|          #|\n",
      "|         godaddy.com|Analytics,Adverti...|          #|\n",
      "|     statcounter.com|           Analytics|StatCounter|\n",
      "|       wordpress.com|                   #|          #|\n",
      "|          paypal.com|                   #|          #|\n",
      "|           twimg.com|     ContentDelivery|    Twitter|\n",
      "+--------------------+--------------------+-----------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "tracker_bucket = 's3://aws-emr-resources-235671948910-us-east-1/labeled_third_party/'\n",
    "df_tracker = spark.read.option(\"header\",True).csv(tracker_bucket)\n",
    "df_tracker.createOrReplaceTempView(\"tracker_list\")\n",
    "sqlDF_tracker = spark.sql(\"SELECT Domain, Category, Company from tracker_list\")\n",
    "sqlDF_tracker.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05952752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803f409d06924b73aeba645b0990be64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              domain|\n",
      "+--------------------+\n",
      "|  123contactform.com|\n",
      "|             2o7.net|\n",
      "|           4stats.de|\n",
      "|               4u.pl|\n",
      "|               51.la|\n",
      "|              a8.net|\n",
      "|     accesstrade.net|\n",
      "|         adbrite.com|\n",
      "|         addthis.com|\n",
      "|        addtoany.com|\n",
      "|           adjug.com|\n",
      "|       admission.net|\n",
      "|          adonweb.ru|\n",
      "|          adriver.ru|\n",
      "|          adroll.com|\n",
      "|          adtaily.pl|\n",
      "|           adtech.de|\n",
      "|adultfriendfinder...|\n",
      "|     advertising.com|\n",
      "|    advertstream.com|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "['123contactform.com', '2o7.net', '4stats.de', '4u.pl', '51.la', 'a8.net', 'accesstrade.net', 'adbrite.com', 'addthis.com', 'addtoany.com', 'adjug.com', 'admission.net', 'adonweb.ru', 'adriver.ru', 'adroll.com', 'adtaily.pl', 'adtech.de', 'adultfriendfinder.com', 'advertising.com', 'advertstream.com', 'affili.net', 'affiliate-b.com', 'affiliatefuture.com', 'akavita.by', 'amazingcounters.com', 'amung.us', 'andyhoppe.com', 'anrdoezrs.net', 'apture.com', 'attracta.com', 'audioacrobat.com', 'aweber.com', 'awempire.com', 'awin1.com', 'awltovhc.com', 'begun.ru', 'belboon.de', 'belstat.be', 'belstat.nl', 'bfast.com', 'bidvertiser.com', 'bigmir.net', 'blogcatalog.com', 'blogtoplist.com', 'boldchat.com', 'browser-update.org', 'btcustomerstreet.com', 'bufferapp.com', 'buttonshut.com', 'buysellads.com', 'cam-content.com', 'casalemedia.com', 'chango.com', 'chart.dk', 'chartbeat.com', 'chitika.net', 'cincopa.com', 'clearspring.com', 'clickbank.net', 'clicksor.com', 'clicktale.net', 'clixgalore.com', 'clustrmaps.com', 'cnzz.com', 'cobaltgroup.com', 'cobaltnitra.com', 'comm100.com', 'compete.com', 'conduit.com', 'connextra.com', 'contactatonce.com', 'contador-de-visitas.com', 'contaxe.com', 'counter-go.de', 'counter-kostenlos.net', 'coveritlive.com', 'cpxinteractive.com', 'cqcounter.com', 'criteo.com', 'daumcdn.net', 'dealer.com', 'demdex.net', 'directtrack.com', 'disclaimer.de', 'disqus.com', 'domdex.com', 'doubleclick.net', 'dpbolvw.net', 'ds1.nl', 'dsultra.com', 'e-junkie.com', 'e-zeeinternet.com', 'e2ma.net', 'easycounter.com', 'ecwid.com', 'ekmpinpoint.com', 'eprize.com', 'ero-advertising.com', 'estat.com', 'etargetnet.com', 'etracker.com', 'etracker.de', 'exoclick.com', 'extreme-dm.com', 'facebook.com', 'facebook.net', 'fastclick.net', 'fastonlineusers.com', 'fbshare.me', 'fc2.com', 'feed2js.org', 'feedburner.com', 'feedjit.com', 'feedsportal.com', 'flagcounter.com', 'flattr.com', 'formstack.com', 'foxyform.com', 'free-counters.co.uk', 'freelogs.com', 'freeonlineusers.com', 'friendfeed.com', 'ftjcfx.com', 'fwdservice.com', 'gemius.pl', 'geovisite.com', 'getclicky.com', 'gigya.com', 'goingup.com', 'google-analytics.com', 'googleadservices.com', 'googlesyndication.com', 'gostats.com', 'gravatar.com', 'haloscan.com', 'hatena.ne.jp', 'hebdotop.com', 'hellobar.com', 'histats.com', 'hit-counts.com', 'hitbox.com', 'hitslink.com', 'hittail.com', 'hitwebcounter.com', 'hotkeys.com', 'hotlog.ru', 'hotwords.com.br', 'hubspot.com', 'humanclick.com', 'i.ua', 'i2i.jp', 'i2idata.com', 'imrworldwide.com', 'infolinks.com', 'informer.com', 'infusionsoft.com', 'intellitxt.com', 'iperceptions.com', 'ivwbox.de', 'jdoqocy.com', 'jscache.com', 'juicyads.com', 'kelkoo.com', 'kitconet.com', 'kontera.com', 'kqzyfj.com', 'lduhtrp.net', 'lijit.com', 'linezing.com', 'linkreferral.com', 'linksalpha.com', 'linksynergy.com', 'linkwithin.com', 'list.ru', 'livechatinc.com', 'liveinternet.ru', 'localtimes.info', 'mail.ru', 'map-generator.net', 'marinsm.com', 'markethealth.com', 'maxmind.com', 'mb01.com', 'mediaplex.com', 'meebo.com', 'metaffiliation.com', 'microad.jp', 'mikle.com', 'mixi.jp', 'mixpanel.com', 'mlcalc.com', 'mongoosemetrics.com', 'moonmodule.com', 'moreover.com', 'motigo.com', 'myaffiliateprogram.com', 'mybloglog.com', 'mycounter.ua', 'mylivesignature.com', 'myopenid.com', 'mypagerank.net', 'myshopify.com', 'mystat.hu', 'nakanohito.jp', 'nedstat.net', 'netvibes.com', 'networkedblogs.com', 'newrelic.com', 'nrelate.com', 'nuffnang.com.my', 'oddcast.com', 'olark.com', 'onestat.com', 'onlinewebstat.com', 'onlywire.com', 'ontoplist.com', 'ooyala.com', 'openstat.net', 'opentracker.net', 'openx.org', 'outbrain.com', 'overture.com', 'partnercash.de', 'pheedo.com', 'phplivesupport.com', 'pimproll.com', 'plista.com', 'polldaddy.com', 'productserve.com', 'profiseller.de', 'projectwonderful.com', 'prweb.com', 'qksrv.net', 'qksz.net', 'qualigo.de', 'rambler.ru', 'ranking-hits.de', 'realtracker.com', 'reformal.ru', 'revolvermaps.com', 'rpxnow.com', 'rssinclude.com', 'salesforce.com', 'sayyac.net', 'scorecardresearch.com', 'semrush.com', 'shareaholic.com', 'shareasale.com', 'sharethis.com', 'shinobi.jp', 'shinystat.com', 'shinystat.it', 'shopify.com', 'shoutmix.com', 'simplehitcounter.com', 'siteheart.com', 'sitemeter.com', 'sitestat.com', 'skimresources.com', 'skitch.com', 'smartadserver.com', 'smartgb.com', 'smowtion.com', 'snap.com', 'snoobi.com', 'socialtwist.com', 'sohu.com', 'spacash.com', 'speedtest.net', 'spylog.com', 'spylog.ru', 'st-hatena.com', 'stat.pl', 'stat24.com', 'statcounter.com', 'statistics.ro', 'stats4free.de', 'superstats.com', 'surveymonkey.com', 'technorati.com', 'thumbshots.org', 'tincan.co.uk', 'tinycounter.com', 'tqlkg.com', 'tradedoubler.com', 'tradetracker.net', 'trafic.ro', 'translateth.is', 'triggit.com', 'truehits.in.th', 'tsviewer.com', 'tweetmeme.com', 'twitter.com', 'twittercounter.com', 'tynt.com', 'typekit.com', 'ucoz.ru', 'uralweb.ru', 'uservoice.com', 'validclick.com', 'valuecommerce.com', 'vgwort.de', 'video-loader.com', 'viglink.com', 'visistat.com', 'visualwebsiteoptimizer.com', 'vivistats.com', 'vpscash.nl', 'w3counter.com', 'web-stat.com', 'webgains.com', 'webgozar.ir', 'webhits.de', 'webmasterplan.com', 'weborama.fr', 'webstat.com', 'webtraxs.com', 'weebly.com', 'woopra.com', 'xiti.com', 'yadro.ru', 'yandex.ru', 'z5x.net', 'zanox-affiliate.de', 'zanox.com', 'zedo.com', 'zemanta.com', 'zopim.com', 'exelator.com', 'assoc-amazon.jp', 'cetrk.com', 'assoc-amazon.com', 'gstatic.com', 'quantserve.com', 'tns-counter.ru', 'googletagservices.com', 'assoc-amazon.de', 'afcyhf.com', 'luxup.ru', 'reinvigorate.net', 'yieldmanager.com', 'tkqlhce.com', 'crwdcntrl.net', 'keywordblocks.com', 'typekit.net', 'urlstats.com', 'widgetsplus.com', 'linkexchange.com', 'bizographics.com', 'ezakus.net', 'assoc-amazon.co.uk', 'assoc-amazon.fr', 'trackalyzer.com', 'widgethost.com', 'rapidcounter.com']"
     ]
    }
   ],
   "source": [
    "tracker_bucket = 's3://aws-emr-resources-235671948910-us-east-1/edu_trackers/'\n",
    "df_tracker = spark.read.option(\"header\",True).csv(tracker_bucket)\n",
    "df_tracker.createOrReplaceTempView(\"tracker_list\")\n",
    "sqlDF_tracker = spark.sql(\"SELECT domain from tracker_list\")\n",
    "print(sqlDF_tracker.show())\n",
    "tracker_list = sqlDF_tracker.select('domain').rdd.flatMap(lambda x:x).collect()\n",
    "print(tracker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e54e060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e0b36d131b4c8fa41c78f90c5f51b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123contactform.com', '2o7.net', '4stats.de', '4u.pl', '51.la', 'a8.net', 'accesstrade.net', 'adbrite.com', 'addthis.com', 'addtoany.com']"
     ]
    }
   ],
   "source": [
    "tracker_list = sqlDF_tracker.select('Domain').rdd.flatMap(lambda x:x).collect()\n",
    "print(tracker_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134071bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f903bc34f14e40f6900ae95821a9d21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Args:\n",
    "    warc_parse_http_header = True\n",
    "    records_processed = None\n",
    "    warc_input_processed = None\n",
    "    warc_input_failed = None\n",
    "    num_input_partitions = 10\n",
    "    num_output_partitions = 5\n",
    "    output = \"s3://zhan-commoncrawl/tag_count_output\"\n",
    "    output_format = \"csv\"\n",
    "    output_compression = \"gzip\"\n",
    "    output_option = []\n",
    "    local_temp_dir = None\n",
    "    spark_profiler = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8c154ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d56438af6244efda870a7352a4a3634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from io import BytesIO\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from warcio.recordloader import ArchiveLoadFailed\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "import tempfile\n",
    "\n",
    "LOGGING_FORMAT = '%(asctime)s %(levelname)s %(name)s: %(message)s'\n",
    "\n",
    "    \n",
    "class JupyterCCSparkJob(object):\n",
    "    \"\"\"\n",
    "    A simple Spark job definition to process Common Crawl data\n",
    "    \"\"\"\n",
    "\n",
    "    name = 'CCSparkJob'\n",
    "\n",
    "    output_schema = StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"val\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # description of input and output shown in --help\n",
    "    input_descr = \"Path to file listing input paths\"\n",
    "    output_descr = \"Name of output table (saved in spark.sql.warehouse.dir)\"\n",
    "\n",
    "    warc_parse_http_header = True\n",
    "\n",
    "    args = None\n",
    "    records_processed = None\n",
    "    warc_input_processed = None\n",
    "    warc_input_failed = None\n",
    "    log_level = 'INFO'\n",
    "    logging.basicConfig(level=log_level, format=LOGGING_FORMAT)\n",
    "\n",
    "\n",
    "    def parse_arguments(self):\n",
    "        \"\"\" Returns the parsed arguments from the command line \"\"\"\n",
    "\n",
    "        description = self.name\n",
    "        if self.__doc__ is not None:\n",
    "            description += \" - \"\n",
    "            description += self.__doc__\n",
    "        args = Args()\n",
    "        return args\n",
    "\n",
    "    def add_arguments(self, parser):\n",
    "        pass\n",
    "\n",
    "    def validate_arguments(self, args):\n",
    "        if \"orc\" == args.output_format and \"gzip\" == args.output_compression:\n",
    "            # gzip for Parquet, zlib for ORC\n",
    "            args.output_compression = \"zlib\"\n",
    "        return True\n",
    "\n",
    "    def get_output_options(self):\n",
    "        return {x[0]: x[1] for x in map(lambda x: x.split('=', 1),\n",
    "                                        self.args.output_option)}\n",
    "\n",
    "    def init_logging(self, level=None):\n",
    "        if level is None:\n",
    "            level = self.log_level\n",
    "        else:\n",
    "            self.log_level = level\n",
    "        logging.basicConfig(level=level, format=LOGGING_FORMAT)\n",
    "\n",
    "    def init_accumulators(self, sc):\n",
    "        self.records_processed = sc.accumulator(0)\n",
    "        self.warc_input_processed = sc.accumulator(0)\n",
    "        self.warc_input_failed = sc.accumulator(0)\n",
    "\n",
    "    def get_logger(self, spark_context=None):\n",
    "        \"\"\"Get logger from SparkContext or (if None) from logging module\"\"\"\n",
    "        if spark_context is None:\n",
    "            return logging.getLogger(self.name)\n",
    "        return spark_context._jvm.org.apache.log4j.LogManager \\\n",
    "            .getLogger(self.name)\n",
    "\n",
    "    def run(self):\n",
    "        self.args = self.parse_arguments()\n",
    "        conf = SparkConf()\n",
    "        conf.set(\"spark.hadoop.validateOutputSpecs\", \"false\")\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.args.spark_profiler:\n",
    "            conf = conf.set(\"spark.python.profile\", \"true\")\n",
    "        \n",
    "        sc = SparkContext.getOrCreate(conf=conf)\n",
    "        \n",
    "        \n",
    "        sqlc = SQLContext(sparkContext=sc)\n",
    "\n",
    "        self.init_accumulators(sc)\n",
    "\n",
    "        self.run_job(sc, sqlc)\n",
    "\n",
    "        if self.args.spark_profiler:\n",
    "            sc.show_profiles()\n",
    "\n",
    "#         sc.stop()\n",
    "\n",
    "    def log_aggregator(self, sc, agg, descr):\n",
    "        self.get_logger(sc).info(descr.format(agg.value))\n",
    "\n",
    "    def log_aggregators(self, sc):\n",
    "        self.log_aggregator(sc, self.warc_input_processed,\n",
    "                            'WARC/WAT/WET input files processed = {}')\n",
    "        self.log_aggregator(sc, self.warc_input_failed,\n",
    "                            'WARC/WAT/WET input files failed = {}')\n",
    "        self.log_aggregator(sc, self.records_processed,\n",
    "                            'WARC/WAT/WET records processed = {}')\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_by_key_func(a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def to_file(self, data):\n",
    "        url = str(data[0])\n",
    "        trackres = ','.join(data[1])\n",
    "        return '\\t'.join([url,trackers])\n",
    "\n",
    "    def run_job(self, sc, sqlc):\n",
    "        \n",
    "        input_data = sc.parallelize(warc_filename_list)\n",
    "        print(input_data)\n",
    "        output = input_data.mapPartitionsWithIndex(self.process_warcs)\\\n",
    "        .reduceByKey(lambda x,y:x)\n",
    "        print(output.count())\n",
    "        print(output.take(100))\n",
    "        columns = ['url','trackers']\n",
    "        df = output.toDF(columns)\n",
    "        df.write.format(\"csv\").mode('overwrite').option(\"header\", \"true\").save(self.args.output)\n",
    "        \n",
    "        self.log_aggregators(sc)\n",
    "    \n",
    "    def process_warcs(self, id_, iterator):\n",
    "        s3pattern = re.compile('^s3://([^/]+)/(.+)')\n",
    "        base_dir = \"/user/\"\n",
    "\n",
    "        # S3 client (not thread-safe, initialize outside parallelized loop)\n",
    "        no_sign_request = botocore.client.Config(\n",
    "            signature_version=botocore.UNSIGNED)\n",
    "        s3client = boto3.client('s3', config=no_sign_request)\n",
    "       \n",
    "        for item in iterator:\n",
    "            warc_filename = item['warc_filename']\n",
    "            offset = item['warc_record_offset']\n",
    "            length = item['warc_record_length']\n",
    "            \n",
    "            crawl_time = item['crawl']\n",
    "            self.warc_input_processed.add(1)\n",
    "           \n",
    "            self.get_logger().info('Reading from S3 {}'.format(warc_filename))\n",
    "            \n",
    "            offset_end = int(offset) + int(length) - 1\n",
    "            byte_range = 'bytes={offset}-{end}'.format(offset=offset, end=offset_end)\n",
    "\n",
    "            warctemp = None\n",
    "            try:\n",
    "                warctemp = s3client.get_object(Bucket='commoncrawl', Key=warc_filename, Range=byte_range)['Body']\n",
    "            except botocore.client.ClientError as exception:\n",
    "                self.get_logger().error(\n",
    "                    'Failed to access {}: {}'.format(warc_filename, exception))\n",
    "                self.warc_input_failed.add(1)\n",
    "                continue\n",
    "            stream = warctemp\n",
    "\n",
    "            no_parse = (not self.warc_parse_http_header)\n",
    "           \n",
    "            try:\n",
    "                archive_iterator = ArchiveIterator(stream,\n",
    "                                                   no_record_parse=no_parse, arc2warc = True)\n",
    "                \n",
    "                for res in self.iterate_records(crawl_time, archive_iterator):\n",
    "\n",
    "                    yield res\n",
    "            except ArchiveLoadFailed as exception:\n",
    "                self.warc_input_failed.add(1)\n",
    "                self.get_logger().error(\n",
    "                    'Invalid WARC: {} - {}'.format(warc_filename, exception))\n",
    "            finally:\n",
    "                stream.close()\n",
    "\n",
    "    def process_record(self,record,crawl_time = None):\n",
    "        raise NotImplementedError('Processing record needs to be customized')\n",
    "\n",
    "    def iterate_records(self, crawl_time, archive_iterator):\n",
    "        \"\"\"Iterate over all WARC records. This method can be customized\n",
    "           and allows to access also values from ArchiveIterator, namely\n",
    "           WARC record offset and length.\"\"\"\n",
    "    \n",
    "        for record in archive_iterator:\n",
    "            rec_type = record.rec_type\n",
    "            url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "            for res in self.process_record(record,crawl_time):   \n",
    "                yield res\n",
    "         \n",
    "            self.records_processed.add(1)\n",
    "            # WARC record offset and length should be read after the record\n",
    "            # has been processed, otherwise the record content is consumed\n",
    "            # while offset and length are determined:\n",
    "            #  warc_record_offset = archive_iterator.get_record_offset()\n",
    "            #  warc_record_length = archive_iterator.get_record_length()\n",
    "\n",
    "    @staticmethod\n",
    "    def is_wet_text_record(record):\n",
    "        \"\"\"Return true if WARC record is a WET text/plain record\"\"\"\n",
    "        return (record.rec_type == 'conversion' and\n",
    "                record.content_type == 'text/plain')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_wat_json_record(record):\n",
    "        \"\"\"Return true if WARC record is a WAT record\"\"\"\n",
    "        return (record.rec_type == 'metadata' and\n",
    "                record.content_type == 'application/json')\n",
    "\n",
    "    @staticmethod\n",
    "    def is_html(record):\n",
    "        \"\"\"Return true if (detected) MIME type of a record is HTML\"\"\"\n",
    "        html_types = ['text/html', 'application/xhtml+xml']\n",
    "        if (('WARC-Identified-Payload-Type' in record.rec_headers) and\n",
    "            (record.rec_headers['WARC-Identified-Payload-Type'] in\n",
    "             html_types)):\n",
    "            return True\n",
    "        for html_type in html_types:\n",
    "            if html_type in record.content_type:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67a4e6f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b528d4ded43422ea52cf910ba7450e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selectolax.parser import HTMLParser\n",
    "import re\n",
    "regex = \"((?<=[^a-zA-Z0-9])(?:https?\\:\\/\\/|[a-zA-Z0-9]{1,}\\.{1}|\\b)(?:\\w{1,}\\.{1}){1,5}(?:com|org|edu|gov|uk|net|ca|de|jp|fr|au|us|ru|ch|it|nl|se|no|es|mil|iq|io|ac|ly|sm){1}(?:\\/[a-zA-Z0-9]{1,})*)\"\n",
    "\n",
    "def get_text_bs(html):\n",
    "    \n",
    "    try:\n",
    "        tree = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "        trackers = []\n",
    "\n",
    "        body = tree.body\n",
    "        if body is None:\n",
    "            return None\n",
    "\n",
    "        for tag in body.select('style'):\n",
    "            tag.decompose()\n",
    "\n",
    "        text = body.get_text(separator='\\n')\n",
    "        \n",
    "        s = body.find_all('script')\n",
    "        for ss in s:\n",
    "            url = ss.get('src')\n",
    "            domain = str(urlparse(url).netloc)\n",
    "            domain = '.'.join(domain.split('.')[-2:])\n",
    "            if len(domain) >= 2 and domain in tracker_list:\n",
    "                trackers.append(domain)\n",
    "                \n",
    "        \n",
    "        a = body.find_all('a')\n",
    "        for aa in a:\n",
    "            url = aa.get('href')\n",
    "            domain = str(urlparse(url).netloc)\n",
    "\n",
    "            domain = '.'.join(domain.split('.')[-2:])\n",
    "            if len(domain) >= 2 and domain in tracker_list:\n",
    "                trackers.append(domain)\n",
    "\n",
    "      \n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return ','.join(list(set(trackers)))\n",
    "\n",
    "def get_text_selectolax(html):\n",
    "    trackers = []\n",
    "    try:\n",
    "        tree = HTMLParser(html)\n",
    "        if tree.body is None:\n",
    "            return None\n",
    "        \n",
    "        for node in tree.tags('style'):\n",
    "            node.decompose()\n",
    "            \n",
    "        #         找到a\n",
    "        for node in tree.css('a,link,script,iframe,img'):\n",
    "            text = node.text()\n",
    "            if (\"google-analytics\" in text):\n",
    "                trackers.append(\"google-analytics.com\")\n",
    "            if 'href' in node.attributes:\n",
    "                url = node.attributes['href']\n",
    "                domain = str(urlparse(url).netloc)\n",
    "#                 domain = '.'.join(domain.split('.')[-2:])\n",
    "                if len(domain) >= 2:\n",
    "                    trackers.append(domain) # 这里我们认为所有的third-party request 都是trackers\n",
    "            if 'src' in node.attributes:             \n",
    "                url = node.attributes['src']\n",
    "                domain = str(urlparse(url).netloc)\n",
    "#                 domain = '.'.join(domain.split('.')[-2:])\n",
    "                if len(domain) >= 2:\n",
    "                    trackers.append(domain)\n",
    "                    \n",
    "            if \"type\" in node.attributes and node.attributes['type'] == 'text/javascript':\n",
    "                \n",
    "                result = re.findall(regex,text)\n",
    "                for url in result:\n",
    "                    domain = str(urlparse(url).netloc)\n",
    "#                     domain = '.'.join(domain.split('.')[-2:])\n",
    "                    if len(domain) >= 2:\n",
    "                        trackers.append(domain)\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return ','.join(list(set(trackers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40bd4425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb2bd3b6c104b4fa65e5653e2163202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "RDD is empty\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 109, in run\n",
      "  File \"<stdin>\", line 145, in run_job\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 58, in toDF\n",
      "    return sparkSession.createDataFrame(self, schema, sampleRatio)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 746, in createDataFrame\n",
      "    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 390, in _createFromRDD\n",
      "    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 361, in _inferSchema\n",
      "    first = rdd.first()\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1381, in first\n",
      "    raise ValueError(\"RDD is empty\")\n",
      "ValueError: RDD is empty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                    \n",
    "class Trackers_extraction_job(JupyterCCSparkJob):\n",
    "    \"\"\" Count HTML tag names in Common Crawl WARC files\"\"\"\n",
    "\n",
    "    name = \"TagCount\"\n",
    "\n",
    "    # match HTML tags (element names) on binary HTML data\n",
    "    html_tag_pattern = re.compile(b'<([a-z0-9]+)')\n",
    "\n",
    "    def process_record(self, record, crawl_time):\n",
    "        \n",
    "        if record.rec_type != 'response':\n",
    "            return\n",
    "        url = record.rec_headers.get_header('WARC-Target-URI')\n",
    "        domain = urlparse(url).netloc\n",
    "        text = record.content_stream().read()\n",
    "        trackers = get_text_selectolax(text)\n",
    "        if url and url.strip() != '': \n",
    "            yield \"#\".join([url,crawl_time]), trackers\n",
    "\n",
    "\n",
    "            \n",
    "job = Trackers_extraction_job()\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b5ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb86ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
